{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grab times and responses from our data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = pd.read_csv('neurotech/sub-01/eeg/sub-01_task-run4_events.tsv', sep = '\\t')\n",
    "starts = events[events['trial_type'] == 788]\n",
    "starts = starts.rename_axis('index1').reset_index()\n",
    "responses = []\n",
    "for i in range(starts.shape[0]):\n",
    "    current = starts.iloc[i]['index1']\n",
    "    df_new = events.drop(range(int(current)))[events['trial_type'] == 801].rename_axis('index1').reset_index().iloc[0, 0]\n",
    "    answers = events.drop(range(int(df_new)))[(events['trial_type'] >= 900) & (events['trial_type'] <= 909)].iloc[0, 2]\n",
    "    responses += [answers - 900]\n",
    "print(starts['onset'].to_list())\n",
    "print(responses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "General helper code for retrieving features from our data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#this cell gives us the features of a particular eeg signal, which is how we analyze it\n",
    "\n",
    "def calculate_entropy(list_values): #this measures the purity of our data, so higher values mean our data is less pure and so we are less likely to draw a good conclusion from it\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    entropy=scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    "\n",
    "def calculate_statistics(list_values): #this calculates a bunch of statistics for our data, including percentiles, median, mean, standard deviation, variance, and root mean squared\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def calculate_crossings(list_values): #this calculates crossing rates for signals, which is essentially just how quickly our signal changes according to certain metrics(for instance, zero_crossings is how often it changes to zero)\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "def get_features(list_values): #this returns the features of our data, a list of the results from all three of our above functions\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "General code to generate train and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#this cell splits our data into a test and a training set\n",
    "def get_train_test(df, y_col, x_cols, ratio):\n",
    "    \"\"\"\n",
    "    This method transforms a dataframe into a train and test set, for this you need to specify:\n",
    "    1. the ratio train : test (usually 0.7)\n",
    "    2. the column with the Y_values\n",
    "    \"\"\"\n",
    "    mask = np.random.rand(len(df)) < ratio\n",
    "    df_train = df[mask]\n",
    "    df_test = df[~mask]\n",
    "\n",
    "    Y_train = df_train[y_col].values\n",
    "    Y_test = df_test[y_col].values\n",
    "    X_train = df_train[x_cols].values\n",
    "    X_test = df_test[x_cols].values\n",
    "    return df_train, df_test, X_train, Y_train, X_test, Y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#here have the loading of our dataset, with both eeg data and corresponding labels\n",
    "#ideally the formatting is that there's a dictionary with classes as keys and eeg matrices as our values, but we can change code as needed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate test and train data specifically for our eeg data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this cell takes our loaded data, extracts the features, and then returns a test and training set\n",
    "list_labels = []\n",
    "list_features = []\n",
    "for k, v in ...(): #the ellipsis is the name of the dictionary that holds our data\n",
    "    yval = list(....keys()).index(k)  #the ellipsis is the name of the dictionary that holds our data\n",
    "    for signal in v:\n",
    "        features = []\n",
    "        list_labels.append(yval)\n",
    "        list_coeff = pywt.wavedec(signal, 'db2')\n",
    "        for coeff in list_coeff:\n",
    "            features += get_features(coeff)\n",
    "        list_features.append(features)\n",
    "df = pd.DataFrame(list_features)\n",
    "ycol = 'y'\n",
    "xcols = list(range(df.shape[1]))\n",
    "df.loc[:,ycol] = list_labels\n",
    "\n",
    "df_train, df_test, X_train, Y_train, X_test, Y_test = get_train_test(df, ycol, xcols, ratio = 0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this cell trains our classifier on the training data and then tests it on the test data, returning the accuracy. We can mess with the parameters of the classifier as needed\n",
    "cls = GradientBoostingClassifier(n_estimators=10000)\n",
    "cls.fit(X_train, Y_train)\n",
    "train_score = cls.score(X_train, Y_train)\n",
    "test_score = cls.score(X_test, Y_test)\n",
    "print(\"The Train Score is {}\".format(train_score))\n",
    "print(\"The Test Score is {}\".format(test_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
